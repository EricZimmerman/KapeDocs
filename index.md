# What is KAPE

Kroll Artifact Parser and Extractor (KAPE) is primarily a triage program that will target a device or storage location, find the most forensically relevant artifacts (based on your needs), and parse them within a few minutes. Because of its speed, KAPE allows investigators to find and prioritize the more critical systems to their case. Additionally, KAPE can be used to collect the most critical artifacts prior to the start of the imaging process. While the imaging completes, the data generated by KAPE can be reviewed for leads, building timelines, etc.

## How KAPE works
KAPE serves two primary functions: 1) collect files and 2) process collected files with one or more programs. By itself, KAPE does not do anything in relation to either of these functions; rather, they are achieved by reading configuration files on the fly and, based on the contents of these files, collecting and processing files. This makes KAPE very extensible in adding or extending functionality.

KAPE uses the concepts of targets and modules to do its work. KAPE comes with a range of default targets and modules for most common operations needed in most forensic exams. These can also be used as examples to follow to make new targets and modules.

At a high level, KAPE works by adding file masks to a queue. This queue is then used to find and copy out files from a source location. For files that are locked by the operating system, a second pass takes place that bypasses the locking. At the end of the process, KAPE will make a copy and preserve metadata about all available files from a source location into a given directory.

The second (optional) stage of processing is to run one or more programs against the collected data. This works by either targeting specific file names or directories. Various programs are run against the files and the output from the programs is then saved in directories named after a category, such as EvidenceOfExecution, BrowserHistory, AccountUsage, and so on.

By grouping things by category, examiners of all levels have a means to discover relevant information regardless of the individual artifact that a piece of information came from. In other words, it is no longer necessary for an examiner to know to process prefetch, shimcache, amcache, userassist, and so on as it relates to evidence of execution artifacts. By thinking categorically and grouping output in the same way, a wider range of artifacts can be leveraged for any given requirement.

<img src="https://github.com/EricZimmerman/KapeDocs/blob/master/Pictures/ProcessArrow.jpg">

![Process overview](https://github.com/EricZimmerman/KapeDocs/blob/master/Pictures/ProcessArrow.jpg)


### Target Collection
Targets are essentially collections of file and directory specifications. KAPE knows how to read these specifications and expand them to files and directories that exist on a target location. Once KAPE has processed all targets and has built a list of files, the list is processed, and each file is copied from the source to the destination directory.
For files that are locked by the operating system and therefore are not able to be copied by regular means, the file is added to a secondary queue. This secondary queue contains all the files that were locked or in use.
After the primary queue is processed, the secondary queue is processed and a different technique, using raw disk reads, is used to bypass the locks. This results in getting a copy of the file as it exists at the source.
Regardless of how the file is copied (either regularly or via raw access), the original timestamps from all directories and the files themselves are reapplied to the destination files. The metadata is also collected into log files as well.

### Module Execution
Like targets, modules are defined using simple properties and are used to run programs. These programs can target anything, including files collected via the target capabilities as well as any other kinds of programs you may want to run on a system from a live response perspective.
For example, if you collected jump lists, a tool like JLECmd could dump the contents of the jump lists to CSV. If you also wanted to collect the output of netstat or ipconfig, you could do so.
Each of these options would be contained in its own module and then grouped together based on commonality between the modules, such as "NetworkLiveResponse" for example.
